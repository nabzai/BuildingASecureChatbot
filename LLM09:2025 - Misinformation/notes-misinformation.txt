Comments on OWASP Top 10 GenAI (https://genai.owasp.org).
LLM09:2025 - Misinformation

Misingformation in the context of GenAI is essentially halucinations and inacurate content provided in a manner 
that appears correct.
If users rely and act on inacurate answers in critical fields such as medical, legal, financial, this can result 
in severe consequences.

In the case of our secure chatbot, the bot queries the OpenAI LLM and provides the output without fact checking 
or cross-referencing or any validation of facts to ensure acuracy. To do so would be outside of the scope of this 
project.
One can add disclaimers noting that the LLM can be wrong on factual questions, similar to the warnings already in 
place for code, however detecting misinformation or halucinations is inherently difficult without referencing 
external trusted data.

Risks:
    users trusting chatbot outputs
    Halucinated content
    Domain-Specific harm (health, finance, legal)

    Mitigations: add disclaimers:
        "The answer is generated by an AI system and may contain errors. Please verify any critical information independently"

    Add Factchecking:
    For example a secondary check validating dates or statistical information from trusted datasets
    Add Confidence Score: in addition to the disclaimer above a confidence score could be added to any output.

    User feedback loop - let users report or flag incorrect or misleading answers.


Some questions to ask when considering misinformation risk:

What is the source of the training data?

How is the accuracy and reliability of the training data verified?

How do you handle data updates or retraining to ensure the model reflects current and accurate information?

Is synthetic data used and if so, how is it controlled to prevent introducing false patterns?

What mechanisms are in place to detect hallucinations in the model’s output?

Is there any way for the model to output information on the topics of health, finance, or legal? How have you tested that?

How is the model tested for edge cases or ambiguous queries?

Are there confidence thresholds or disclaimers for outputs  where the model has low certainty?

How do you validate the accuracy of the model’s outputs before they reach the user?

Are outputs cross-referenced with known external sources of knowledge to ensure accuracy?

If we assume compromise and most inaccurate output imaginable what is the maximum damage that  a user acting on that output could experience?

Who is responsible for monitoring and addressing Misinformation based risks?